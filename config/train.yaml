model:
  # ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']
  name: RN50x16
  prompt_learning: false
  prompt_from_visual_tokens: false
  qformer_config:
    dim: 1024
    ffn_exp_ratio: 4
    drop_rate: !!float 1e-1
    heads: 8
    depth: 12
  n_prompt: 1
  logit_scale: !!float 0.  # Consider exp(0) = 1
  # Since below paper said that large logit_scale works like triplet loss without margin.
  # For default(pre-trained) logit_scale, use 4.6052
  m2_loss_weight: null
  # https://github.com/changdaeoh/multimodal-mixup

dataset:
  dataset_path: /home/dslisleedh/NLPProject/hpitp_dataset
  permute_colors: False
  to_full_sentence: True
  img_size: 224

train:
  device: cuda:0
  seed: 42
  batch_size: 16
  num_workers: 4
  epochs: 50

# Which is same as BLIP2 paper only change optimizer(AdamW -> AdaBelief)
# Since AdamW + FP16 is not stable which causes NAN Grad we used AdaBelief
optimizer:
  name: AdaBelief
  lr: !!float 1e-4
  betas: [0.9, 0.98]
  weight_decay: !!float 5e-2
  rectify: False

scheduler: 
  name: LinearWarmupCosineAnnealingLR
  warmup_iter: 3
  max_iter: ${train.epochs}
  eta_min: !!float 5e-5
  start_factor: 1e-3

early_stopping:
  criterion: recall_at_1
  patience: 5
  maximize: True

defaults:
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

# hydra and directory-related settings.
# log_prefix: ./train_logs
log_prefix: ./train_logs
hydra:
  run:
    dir: ${log_prefix}/${model.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: ${log_prefix}/${model.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ''

  job:
    chdir: True
    # env_set:
    #   CUDA_VISIBLE_DEVICES: 3
    #   # API_KEY: 1234.....